{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJt0iAf64hxzOJ3W19EEVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojakedia/EnglishToPython/blob/main/EnglishToPythonTransformerModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HWsiwbX9Eya"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import tokenize\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Setting the device for model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the data\n",
        "!wget \"https://drive.google.com/u/0/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\" -O english_python_data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJoiTnCu9bez",
        "outputId": "a5b30dbc-e205-458a-db0f-1c84946cc5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-16 03:44:20--  https://drive.google.com/u/0/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.137.102, 74.125.137.113, 74.125.137.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.137.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://drive.google.com/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download [following]\n",
            "--2024-01-16 03:44:20--  https://drive.google.com/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\n",
            "Reusing existing connection to drive.google.com:443.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download [following]\n",
            "--2024-01-16 03:44:20--  https://drive.usercontent.google.com/download?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1122316 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘english_python_data.txt’\n",
            "\n",
            "english_python_data 100%[===================>]   1.07M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-01-16 03:44:21 (8.19 MB/s) - ‘english_python_data.txt’ saved [1122316/1122316]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examining the dataset\n",
        "with open('english_python_data.txt',\"r\") as data_file:\n",
        "  print(data_file.readlines()[:5]) # Printing out the first 5 lines of the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DM33_BB9emN",
        "outputId": "23f82065-ffab-45e7-ccd3-4adbe87f3b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['# write a python program to add two numbers \\n', 'num1 = 1.5\\n', 'num2 = 6.3\\n', 'sum = num1 + num2\\n', \"print(f'Sum: {sum}')\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Regular Expression (Regex) pattern of urls to remove them\n",
        "url_pattern = re.compile(r\"https?://\\S+\")\n",
        "\n",
        "# Making a dataset\n",
        "with open('english_python_data.txt',\"r\") as data_file:\n",
        "  data_lines = data_file.readlines()\n",
        "  dps = [] # List of dictionaries\n",
        "  dp = None # The current problem and solution\n",
        "  for line in data_lines:\n",
        "    if line[0] == \"#\":\n",
        "      if dp:\n",
        "        dp['solution'] = ''.join(dp['solution'])\n",
        "        dps.append(dp)\n",
        "      dp = {\"question\": None, \"solution\": []}\n",
        "      dp['question'] = line[1:].strip(\"\\n \") # Removing any \\n in the question\n",
        "      dp['question'] = re.sub(r'^\\d+ ', \"\", dp['question']) # If the question starts with numbers, I remove them.\n",
        "      dp['question'] = url_pattern.sub('',dp['question']) # Replacing any urls\n",
        "      dp['question'] = dp['question'].lower() # lowercasing the question\n",
        "      dp['question'] = re.sub(r\"([.!?])\",\"\",dp['question']) # removing any punctuation\n",
        "    else:\n",
        "      dp[\"solution\"].append(line)\n",
        "\n",
        "# converting the data to a table for easier viewing\n",
        "dataset = pd.DataFrame(dps)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ampdiat69ghX",
        "outputId": "26de9b82-4a8f-477d-d9c8-591d980eff6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               question  \\\n",
              "0             write a python program to add two numbers   \n",
              "1     write a python function to add two user provid...   \n",
              "2     write a program to find and print the largest ...   \n",
              "3     write a program to find and print the smallest...   \n",
              "4     write a python function to merge two given lis...   \n",
              "...                                                 ...   \n",
              "4952  write a program to print bit wise and of two n...   \n",
              "4953  write a program to print bit wise or of two nu...   \n",
              "4954  write a program to print bit wise xor of two n...   \n",
              "4955  write a program to calculate binary ones compl...   \n",
              "4956      write a program to binary left shift a number   \n",
              "\n",
              "                                               solution  \n",
              "0     num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...  \n",
              "1     def add_two_numbers(num1, num2):\\n    sum = nu...  \n",
              "2     \\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >=...  \n",
              "3     num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...  \n",
              "4     def merge_lists(l1, l2):\\n    return l1 + l2\\n...  \n",
              "...                                                 ...  \n",
              "4952  a = 60            # 60 = 0011 1100\\nb = 13    ...  \n",
              "4953  a = 60\\nb = 13\\n\\nc = a | b\\nprint(\"OR\", c)\\n\\n\\n  \n",
              "4954  a = 60\\nb = 13\\n\\nc = a ^ b\\nprint(\"XOR\", c)\\n...  \n",
              "4955  a = 60\\n\\nc = ~a\\nprint(\"Binary Ones Complemen...  \n",
              "4956    c = a << 2\\nprint(\"Binary Left Shift\", c)\\n\\n\\n  \n",
              "\n",
              "[4957 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e53226f9-5771-4eff-b9aa-9a696200da6e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>write a python program to add two numbers</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python function to add two user provid...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a program to find and print the largest ...</td>\n",
              "      <td>\\nnum1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the smallest...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a python function to merge two given lis...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4952</th>\n",
              "      <td>write a program to print bit wise and of two n...</td>\n",
              "      <td>a = 60            # 60 = 0011 1100\\nb = 13    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4953</th>\n",
              "      <td>write a program to print bit wise or of two nu...</td>\n",
              "      <td>a = 60\\nb = 13\\n\\nc = a | b\\nprint(\"OR\", c)\\n\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4954</th>\n",
              "      <td>write a program to print bit wise xor of two n...</td>\n",
              "      <td>a = 60\\nb = 13\\n\\nc = a ^ b\\nprint(\"XOR\", c)\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4955</th>\n",
              "      <td>write a program to calculate binary ones compl...</td>\n",
              "      <td>a = 60\\n\\nc = ~a\\nprint(\"Binary Ones Complemen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4956</th>\n",
              "      <td>write a program to binary left shift a number</td>\n",
              "      <td>c = a &lt;&lt; 2\\nprint(\"Binary Left Shift\", c)\\n\\n\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4957 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e53226f9-5771-4eff-b9aa-9a696200da6e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e53226f9-5771-4eff-b9aa-9a696200da6e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e53226f9-5771-4eff-b9aa-9a696200da6e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-71255383-48b6-465e-a29a-9fab19a32b84\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-71255383-48b6-465e-a29a-9fab19a32b84')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-71255383-48b6-465e-a29a-9fab19a32b84 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the first question and the corresponding solution\n",
        "print(dataset.loc[0,'question'])\n",
        "print(dataset.loc[0,'solution'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ1PN0P-9pAb",
        "outputId": "b5d59377-4e5c-4786-a8b6-f4a35cdf432b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write a python program to add two numbers\n",
            "num1 = 1.5\n",
            "num2 = 6.3\n",
            "sum = num1 + num2\n",
            "print(f'Sum: {sum}')\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dictionaries for the tokenizers and the vocabularies\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'python'\n",
        "\n",
        "tokenizers = {}\n",
        "vocabularies = {}\n",
        "\n",
        "tokenizers[SRC_LANGUAGE] = get_tokenizer('spacy',language='en_core_web_sm')\n",
        "tokenizers[TGT_LANGUAGE] = get_tokenizer('spacy',language='en_core_web_sm')\n",
        "\n",
        "# Generating special characters and their indicies\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # Tokens for Unknown, Padding, beginning of sentence, end of sentence\n",
        "special_symbols = ['<unk>','<pad>','<bos>','<eos>']\n",
        "\n",
        "# Generating the tokens\n",
        "# This function returns\n",
        "def yield_tokens(data_iter, language):\n",
        "  for data_sample in data_iter:\n",
        "    yield tokenizers[SRC_LANGUAGE](data_sample)\n",
        "\n",
        "# Generating the tokens and making the vocabularies based off of them\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  if language == SRC_LANGUAGE:\n",
        "    vocabularies[language] = build_vocab_from_iterator(yield_tokens(dataset['question'],language),min_freq=1,specials=special_symbols,special_first=True)\n",
        "  else:\n",
        "    vocabularies[language] = build_vocab_from_iterator(yield_tokens(dataset['solution'],language),min_freq=1,specials=special_symbols,special_first=True)\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocabularies[ln].set_default_index(UNK_IDX) # Setting the unknown index to be the default index when a token isn't found"
      ],
      "metadata": {
        "id": "FvQU2Zu49q1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing how big our vocabularies are\n",
        "print(vocabularies[SRC_LANGUAGE].__len__())\n",
        "print(vocabularies[TGT_LANGUAGE].__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhmmVYNW9u9j",
        "outputId": "d2731571-dccc-4f3b-dbe9-8d607a22d582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2387\n",
            "11880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking up a word in the vocabulary\n",
        "print(vocabularies[SRC_LANGUAGE]['write'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1iLaof096Af",
        "outputId": "8aeade77-e224-4eca-fb61-7822aedd53d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Positional Encoding module -> this class is the positional encoder (see above for details)\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,emb_size:int, dropout:float, maxlen:int = 5000):\n",
        "    super(PositionalEncoding,self).__init__()\n",
        "    den = torch.exp(-torch.arange(0,emb_size,2)*math.log(10000) / emb_size)\n",
        "    pos = torch.arange(0,maxlen).reshape(maxlen,1)\n",
        "    pos_embedding = torch.zeros((maxlen,emb_size))\n",
        "    pos_embedding[:,0::2] = torch.sin(pos * den)\n",
        "    pos_embedding[:,1::2] = torch.cos(pos * den)\n",
        "    pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Saving the positional encoding in the model state dict, but making sure PyTorch doesn't \"train\"\n",
        "    # these parameters because they don't need to be trained\n",
        "    self.register_buffer('pos_embedding',pos_embedding)\n",
        "\n",
        "  def forward(self,token_embedding: Tensor):\n",
        "    return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# Converting the tokens into embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size: int, emb_size):\n",
        "    super(TokenEmbedding, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,emb_size)\n",
        "    self.embed_size = emb_size\n",
        "\n",
        "  def forward(self,tokens:Tensor):\n",
        "    return self.embedding(tokens.long()) * math.sqrt(self.embed_size) # we multiply by square root of embedding size to scale. The Transformer paper mentions this.\n",
        "\n",
        "# The Actual Model\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "  def __init__(self, num_encoder_layers:int, num_decoder_layers:int, emb_size:int, nhead:int, src_vocab_size:int, tgt_vocab_size:int, dim_feedforward: int=512, dropout:float = 0.1):\n",
        "    super(Seq2SeqTransformer, self).__init__()\n",
        "    self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers,dim_feedforward=dim_feedforward,dropout=dropout,\n",
        "                                      batch_first=True)\n",
        "    self.generator = nn.Linear(emb_size,tgt_vocab_size) # A layer to convert the matrix (seq_len, emb_size) to (seq_len, tgt_vocab_size)\n",
        "    self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "    self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size,emb_size)\n",
        "\n",
        "    # Getting the positional encodings\n",
        "    self.positional_encoding = PositionalEncoding(emb_size,dropout=dropout)\n",
        "\n",
        "  def forward(self, src:Tensor, trg: Tensor, src_mask: Tensor, tgt_mask: Tensor, src_padding_mask: Tensor, tgt_padding_mask: Tensor,\n",
        "              memory_key_padding_mask: Tensor):\n",
        "\n",
        "    # Embedding both the input and output\n",
        "    src_embedding = self.positional_encoding(self.src_tok_emb(src))\n",
        "    tgt_embedding = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "\n",
        "    # Getting the output\n",
        "    output = self.transformer(src_embedding, tgt_embedding, src_mask, tgt_mask, None, src_padding_mask,tgt_padding_mask,memory_key_padding_mask)\n",
        "\n",
        "    # Getting the logits\n",
        "    return self.generator(output)\n",
        "\n",
        "  # Encoding the input\n",
        "  def encode(self, src: Tensor, src_mask: Tensor):\n",
        "    embedding = self.positional_encoding(self.src_tok_emb(src))\n",
        "    encoder_output = self.transformer.encoder(embedding, src_mask)\n",
        "    return encoder_output\n",
        "\n",
        "  # Decoding the output\n",
        "  def decode(self,tgt:Tensor, memory: Tensor, tgt_mask:Tensor):\n",
        "    tgt_embedding = self.tgt_tok_emb(tgt)\n",
        "    return self.transformer.decoder(self.positional_encoding(tgt_embedding),memory, tgt_mask)\n"
      ],
      "metadata": {
        "id": "Oe4_Do4F977m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the lookahead mask that will prevent the model from looking ahead during training\n",
        "# Also need to define masks that will mask the padding tokens.\n",
        "# If we don't mask the padding tokens, the model will end up taking the values of the padding into account\n",
        "# into prediction\n",
        "\n",
        "# Generating the lookahead mask\n",
        "def generate_square_subsequent_mask(sz):\n",
        "  mask = (torch.triu(torch.ones((sz,sz),device=DEVICE)) == 1).transpose(0,1)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "  return mask\n",
        "\n",
        "# Creating the other mask\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[1]\n",
        "  tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "  src_padding_mask = (src == PAD_IDX)\n",
        "  tgt_padding_mask = (tgt == PAD_IDX)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "YxGNDIeO-A0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing\n",
        "training, testing = train_test_split(dataset,test_size=0.2,random_state=42,shuffle=True)\n",
        "\n",
        "# Running the data through a pipeline to get the transformed and prepared dataset\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "  def func(txt_input):\n",
        "    for transform in transforms:\n",
        "      txt_input = transform(txt_input)\n",
        "    return txt_input\n",
        "  return func\n",
        "\n",
        "# Function to add BOS/EOS and create tensor for input sequence indicies\n",
        "def tensor_transform(token_ids):\n",
        "  return torch.cat((torch.tensor([BOS_IDX]),torch.tensor(token_ids),torch.tensor([EOS_IDX])))\n",
        "\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  text_transform[ln] = sequential_transforms(tokenizers[ln],vocabularies[ln],tensor_transform) # Tokenize, Convert to Indicies, then Add Special Tokens\n",
        "\n",
        "# function to put all the data samples into batches\n",
        "def collate_fn(batch):\n",
        "  src_batch, tgt_batch = [], []\n",
        "\n",
        "  # Iterating through the questions\n",
        "  for X in batch['question'].values:\n",
        "    src_batch.append(text_transform[SRC_LANGUAGE](X.strip('\\n\\t')))\n",
        "\n",
        "  # Iterating through the solutions\n",
        "  for y in batch['solution'].values:\n",
        "    tgt_batch.append(text_transform[TGT_LANGUAGE](y.strip('\\n\\t')))\n",
        "\n",
        "  src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "  tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "  return src_batch.T, tgt_batch.T"
      ],
      "metadata": {
        "id": "Qr8LIAmp-DEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model, loss function, and optimizer\n",
        "torch.manual_seed(10)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocabularies[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocabularies[TGT_LANGUAGE])\n",
        "EMB_SIZE = 128\n",
        "NHEAD = 4\n",
        "FFN_HID_DIM = 128\n",
        "BATCH_SIZE = 15\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "# Defining the model\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "# Setting the parameters using the xavier uniform distribution\n",
        "for p in transformer.parameters():\n",
        "  if p.dim() > 1:\n",
        "    nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Putting the model on GPU\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "# Defining the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) # makes sure that the padding token doesn't contribute to the loss function!\n",
        "\n",
        "# Defining the optimizer\n",
        "optimizer = torch.optim.Adam(transformer.parameters(),lr=0.0001)"
      ],
      "metadata": {
        "id": "FSVMdrrq-ICb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model,optimizer):\n",
        "  # Setting the model to training mode\n",
        "  model.train()\n",
        "  losses = 0\n",
        "\n",
        "  # Preparing the data\n",
        "  X,y = collate_fn(training)\n",
        "  training_dataset = TensorDataset(X,y)\n",
        "  train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # Iterating through the data\n",
        "  for src, tgt in train_dataloader:\n",
        "    src = src.to(DEVICE)\n",
        "    tgt = tgt.to(DEVICE)\n",
        "\n",
        "    tgt_input = tgt[:,:-1] # Getting the sentence except the EOS since EOS is never inputted to decoder\n",
        "\n",
        "    # Getting the masks\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "    logits = model(src,tgt_input, src_mask, tgt_mask, src_padding_mask,tgt_padding_mask,src_padding_mask) # memory is the encoder outputs\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    tgt_out = tgt[:,1:]\n",
        "    loss = loss_fn(logits.reshape(logits.size(0),-1,1827),tgt_out)\n",
        "    loss.backward() # Back propagation, calculating the gradients\n",
        "\n",
        "    optimizer.step()\n",
        "    losses += loss.item()\n",
        "\n",
        "  return losses / len(list(train_dataloader)) # Getting the average loss per example\n",
        "\n",
        "# Evaluation Loop\n",
        "def evaluate(model):\n",
        "  model.eval()\n",
        "  losses = 0\n",
        "\n",
        "  # Preparing the data\n",
        "  X,y = collate_fn(testing)\n",
        "  testing_data = TensorDataset(X,y)\n",
        "  val_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # Iterating through the data\n",
        "  for src, tgt in val_dataloader:\n",
        "    src = src.to(DEVICE)\n",
        "    tgt = tgt.to(DEVICE)\n",
        "\n",
        "    tgt_input = tgt[:,:-1] # Getting the sentence except the EOS since EOS is never inputted to decoder\n",
        "\n",
        "    # Getting the masks\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "    logits = model(src,tgt_input, src_mask, tgt_mask, src_padding_mask,tgt_padding_mask,src_padding_mask) # memory is the encoder outputs\n",
        "    tgt_out = tgt[:,1:]\n",
        "    loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt_out.reshape(-1))\n",
        "    losses += loss.item()\n",
        "\n",
        "  return losses / len(list(val_dataloader)) # Getting the average loss per example"
      ],
      "metadata": {
        "id": "4Ff3Hrfs-Kjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(1,NUM_EPOCHS+1):\n",
        "  start_time = timer()\n",
        "  train_loss = train_epoch(transformer, optimizer)\n",
        "  end_time = timer()\n",
        "  val_loss = evaluate(transformer)\n",
        "  print(f'Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}')\n",
        "  print(f'Epoch time: {(end_time - start_time):.3f}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHNMLhPh-ZWq",
        "outputId": "77f0ebd9-cacb-4ae4-f382-d57f9f79d48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 8.633, Val loss: 8.743\n",
            "Epoch time: 132.208s\n",
            "Epoch: 2, Train loss: 7.844, Val loss: 8.927\n",
            "Epoch time: 131.146s\n",
            "Epoch: 3, Train loss: 6.660, Val loss: 9.081\n",
            "Epoch time: 130.989s\n",
            "Epoch: 4, Train loss: 5.932, Val loss: 9.217\n",
            "Epoch time: 131.456s\n",
            "Epoch: 5, Train loss: 5.661, Val loss: 9.231\n",
            "Epoch time: 130.988s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to generate the output sequence autoregressively using the greedy decoder algorithm\n",
        "def greedy_decode(model, src, max_len, start_symbol):\n",
        "  src = src.to(DEVICE)\n",
        "  memory = model.encode(src.view(1,-1), None)\n",
        "  memory = memory.to(DEVICE)\n",
        "  ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "  for i in range(max_len-1):\n",
        "      out = model.decode(ys.view(1,-1), memory, None)\n",
        "      prob = nn.functional.softmax(model.generator(out[:, -1]),dim=1)\n",
        "      _, next_word = torch.max(prob, dim=1)\n",
        "      next_word = next_word.item()\n",
        "\n",
        "      ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "      if next_word == EOS_IDX:\n",
        "          break\n",
        "  return ys\n",
        "\n",
        "# Function for translation\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "  model.eval()\n",
        "  src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "  num_tokens = src.shape[0]\n",
        "  tgt_tokens = greedy_decode(model,  src, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "  return \" \".join(vocabularies[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "gUcoFNMlXpbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying to translate from English to Python\n",
        "print(translate(transformer, \"Give me a function to add 3 numbers.\"))"
      ],
      "metadata": {
        "id": "RWxJUQV0XsZL",
        "outputId": "b6591097-12a0-47b3-828a-18131a9e7fb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " isLucky(next_position isLucky(next_position isLucky(next_position zip(*test_dict.values y':5874 y':5874 y':5874 rel_tol rel_tol rel_tol d.append(a king king king king\n"
          ]
        }
      ]
    }
  ]
}